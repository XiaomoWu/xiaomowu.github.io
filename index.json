[{"categories":null,"content":" 1 TL;DRNailing down the earnings announcement time (ideally to the seconds or milliseconds) is critical to measure market response. However, many databases usually don’t provide a precise event timestamp. For those who do, the data quality is questionable, if not confusing. In this article, I’ll investigate the following four popular databases and tell you which offers the most precise timestamp of earnings events. The databases and the timestamp variables we’ll look at (All of them are available at WRDS) are: Compustat (from S\u0026P) RDQ (report date quarterly) is offered by Compustat to indicate “the earliest time EPS data is public”. RDQ is a Date, e.g., 2001-01-01 Key Development from Capital IQ (hereafter CIQ, also from S\u0026P) mostImportantDateUTC is a DateTime (e.g., 2001-01-01 12:24:00) precise to the minute (I’ll explain later why it’s not on the second level). To use it you need to convert it to Eastern Time. WRDS SEC Analytic Suite (from SEC and compiled by WRDS) fdate (filing date), secadate/secatime (EDGAR accepted date/time) are collected EDGAR. You can find them in the wrds_forms table from WRDS SEC Analytics Suite. RavenPack (hereafter RP) timestamp_utc is the news timestamp to the milliseconds. It’s the only variable that offers millisecond-level data. Guide The filing date (fdate) and accepted datetime (secadate and secatime) from SEC are not the first public time. They have lags. The rdq from Compustat also has lags The mostImportantDateUTC from Capital IQ is better but still has some issues. If you don’t want to bother with RavenPack, I suggest you use CIQ in the following way: If the time component of mostImportantDateUTC is zero (e.g., ‘2001-01-01 00:00:00’), then treat ostImportantDateUTC` as the Eastern Time. In this case, you can’t pin down the precise time of the day but the date should be correct. If it does has a none-zero time component (e.g., ‘2001-01-01 12:03:00’), converting mostImportantDateUTC to ET is safe and recommended. Your best bet is combining RavenPack and CIQ , but it requires a lot of data cleaning. ","date":"2023-01-25","objectID":"/nail-down-earnings-time/:1:0","series":["WRDS Tips"],"tags":["wrds","ravenpack"],"title":"Nailing down the earnings event time to the millisecond\n","uri":"/nail-down-earnings-time/#tldr"},{"categories":null,"content":" 2 Background: How does the earnings news arrive at you?We all know the earnings data (e.g., EPS) is from the company. But how soon this data arrives to you heavily depends on the channel. Typically, the earnings data is released and spread in this order: A company releases information to all parties at roughly the same time. To SEC, that will be an EDGAR filing, to the media; it’s a written press release. Note that what the company submits to EDGAR at this time is an 8-K, not 10-Q. The 8-K only covers critical profits and operation data. The more detailed 10-Q is usually filed days later. SEC (EDGAR) takes some time to process the filing before it accepts and publishes it online, creating a lag. If the filing is received before 5:30 pm ET, then EDGAR will process it in (usually) less than two minutes and release it afterwards. If the filing is received after 5:30 pm, then normally it will have a filing date of the next trading day (See Section 3.2 of this official doc). In this case, the filing date will be one day after the actual event date. Media outlets, however, are not obliged to validate the information so they may release the news before EDGAR. The lead of media report could be in minutes or even hours. ","date":"2023-01-25","objectID":"/nail-down-earnings-time/:2:0","series":["WRDS Tips"],"tags":["wrds","ravenpack"],"title":"Nailing down the earnings event time to the millisecond\n","uri":"/nail-down-earnings-time/#background-how-does-the-earnings-news-arrive-at-you"},{"categories":null,"content":" 3 SEC filing date has a lagAs discussed above, SEC filing date may be later than the media. Here I provide an example. In the screenshot of EDGAR (SEC accession No. 0001171843-15-001360) below, the accepted time is before the EDGAR close time (5:30 pm ET), so the filing date (fdate) and accepted date (secadate) is on the same day. Filing date is on the same day as the accepted date In contrast, in the following screenshot, filing date is one day after the accepted date since it’s submitted after 5:30 pm ET: Filing date is one day later than accepted date The official manual of EDGAR also says EDGAR only accept filings between 6 am and 10 pm ET. As a results, the accepted time of any filing should also be within this range. I checked and confirmed that. (There’re a few exceptions whose accepted date is around 00:00:00 though) Distribution of accepted time To summary, SEC dates are generally later than media due to the limitations of EDGAR. Some `secadate` and `fdate` are years gapped! You may find some secadate are years after the fdate: The reason is that some filings are initially filed by paper and later converted to digital format by EDGAR. Refer to my previous post “The “Accepted Date” from WRDS SEC suite doesn’t match EDGAR” for detail. The issue will be almost gone for recent filings: For fdate\u003e='2002-05-01' and forms in ['8-K', '10-Q', '10-K'], the fdate of 98.1% filings is on the same day or one day behind the secadate. ","date":"2023-01-25","objectID":"/nail-down-earnings-time/:3:0","series":["WRDS Tips"],"tags":["wrds","ravenpack"],"title":"Nailing down the earnings event time to the millisecond\n","uri":"/nail-down-earnings-time/#sec-filing-date-has-a-lag"},{"categories":null,"content":" 4 RDQ is good, but it has no time componentRDQ, the Report Date Quarterly, is described to be “the date on which quarterly earnings per share are first publicly reported.” RDQ is accurate, I found 98% of RDQs aggree with other data sources. However, the missing time component makes it impossible to accurately time the market response: If the earnings news is released before market close (4 pm ET), then day 0 is the release date; if news is released after 4 pm ET, then day 0 should be the next trading day! That would make a big difference since a significant portion of market response is on day 0. Sadly, RDQ doesn’t tell us if the earnings news is releaed before or after the market close. S\u0026P client support answers how RDQ is collected RDQ in Compustat is collected as a blank field until the 10Q or the related Newswire is filed within SEC or a regulatory body if the preliminary news releases do not include earnings. ","date":"2023-01-25","objectID":"/nail-down-earnings-time/:4:0","series":["WRDS Tips"],"tags":["wrds","ravenpack"],"title":"Nailing down the earnings event time to the millisecond\n","uri":"/nail-down-earnings-time/#rdq-is-good-but-it-has-no-time-component"},{"categories":null,"content":" 5 mostImportantDateUTC in CIQ is good, but not perfectmostImportantDateUTC from CIQ seems to track event more closely. From the same S\u0026P client support I learned that: Quote CIQ collects data from the announcement of the company websites, press releases or any publicly available source and companies either post it in their website first before it being available in SEC. So theoretically CIQ can recored events before it’s officially released by EDGAR. Sounds good, right? Unfortunately, mostImportantDateUTC has a three drawbacks: The time component of some events are missing In 25.6% of the case, the datetime will be something like ‘2008-09-01 00:00:00’, where the ‘00:00:00’ indicates CIQ only records the date. It’s floored to the minute I check the whole CIQ database and find the “second” component are all zero. That means if the true event time is “17:30:43”, CIQ will floor it to “17:30:00”. It’s not necessarily the “earliest” timestamp After comparing to RavenPack, I found mostImportDateUTC is not always the earliest timestamp. There’re many cases where the earliest news article in RP is earlier than CIQ. As a summary, if you don’t want to bother with digging the RavenPack data, I suggest you use CIQ in the following way: How to use CIQ (`mostImportantDateUTC`) smartly If the time component of mostImportantDateUTC is zero (e.g., ‘2001-01-01 00:00:00’), then treat mostImportantDateUTC as if it’s in Eastern Time (i.e., ‘2001-01-01 UTC’ becomes ‘2001-01-01 ET’). If it does has a none-zero time component (e.g., ‘2001-01-01 12:03:00’), converting mostImportantDateUTC to ET is safe and recommended (i.e., ‘2001-01-01 12:03:00 UTC’ becomes ‘2001-01-01 07:03:00 EST’). ","date":"2023-01-25","objectID":"/nail-down-earnings-time/:5:0","series":["WRDS Tips"],"tags":["wrds","ravenpack"],"title":"Nailing down the earnings event time to the millisecond\n","uri":"/nail-down-earnings-time/#mostimportantdateutc-in-ciq-is-good-but-not-perfect"},{"categories":null,"content":" 6 Combine RavenPack and Capital IQAll news articles in RavenPack are timed to the milliseconds. This makes RP a perfect source for the timing of earnings event. However, since RP focuses on news articles, it’s possible that some events may not be covered by any news media. In addition, RP is big, with one year’s data over 200GB, and its taxonomy on earnings events are very complex (there’re \u003e20 event types that are all about earnings). This makes matching RP to other databases very difficult. So your best shot is to join the merits of both: supplement CIQ with RavenPack. The general idea is: If we can find an earlier timestamp in RP, then we use RP; otherwise, we stick with CIQ. On average, 54% of CIQ timestamps can be improved by RavenPack. ","date":"2023-01-25","objectID":"/nail-down-earnings-time/:6:0","series":["WRDS Tips"],"tags":["wrds","ravenpack"],"title":"Nailing down the earnings event time to the millisecond\n","uri":"/nail-down-earnings-time/#combine-ravenpack-and-capital-iq"},{"categories":null,"content":" 6.1 The logic of codeCombining RP with CIQ is not easy. I first present the logic of the code (so you can implement with other languages) and in the next section, I provide an R example. Step 1: Split CIQ You start with a table called ciq. This could be the raw “Key Development” table in CIQ or any cleaned table. The only requirement is that each line of ciq represents one earnings event. The most important column in ciq would be mostImportantDateUTC – the event timestamp provided by CIQ. Split table ciq into two tables according to if mostImportantDateUTC has a time component or not: table ciqd if mostImportantDateUTC only has the date component, and table ciqdt if it has both a date and a time component. Step 2: Process ciqd Since there’s no time component, we’ll create a default one. It equals to the last second of the RDQ day. That is, if RDQ is 2001-01-01, the default timestamp would be 2001-01-01 23:59:59. Join ciqd to the RavenPack table: For every earnings event in ciqd, we query the previous 2 days in RP. If we can find a match, then we update the event time from the default (23:59:59) to the RP timestamp. Step 3: Process ciqdt For each event in ciqdt, query the previous 12 hours in RP. If there’s a match, label the retrived RP timestamp as \"match_prev\". Similarly for the same event, query the next 60s in RP. Label the retrieved RP timestamp as \"match_next\". This step is to deal with the “flooring second” issue. Now each event in ciqdt has three status depending on what type of RP timestamp it successfully retrieved: Failed to match any RP timestamp. We then use the CIQ timestamp (mostImportantDateUTC) as the event time. At least matches to one \"match_prev\" timestamp (also including the case where it matches to both \"match_prev\" and \"match_next\" timestamps). This means we successfully find an “earlier” timestamp in RP. We then use the \"match_prev\" timestamp as the event time. Only matches to a \"match_next\" timestamp. This means the CIQ timestamp is earlier than any other media by no more than 60 seconds. It’s a indicator of the “flooring issue.” In this case, we use the \"match_next\" timestamp. Step 4: Combine ciqd and ciqdt ","date":"2023-01-25","objectID":"/nail-down-earnings-time/:6:1","series":["WRDS Tips"],"tags":["wrds","ravenpack"],"title":"Nailing down the earnings event time to the millisecond\n","uri":"/nail-down-earnings-time/#the-logic-of-code"},{"categories":null,"content":" 6.2 R CodeThe R code is not fully reproducible since I omitted hundreds of lines of data cleaning code. But the main logic is untouched. I relied heaveily on the data.table package: you’ll write way more lines of code if you use its competitors like dplyr or pandas. library(stringr) library(data.table) library(lubridate) library(hms) # ---- Define the tables and columns we'll use ---- # - ciq: This could be the raw \"Key Development\" table in CIQ or any cleaned table. # The only requirement is that each line of `ciq` represents *one earnings event*. # The most important column in `ciq` would be `ciq_ann_dttm_utc` -- the event timestamp # provided by CIQ (it's just a rename of `mostImportantDateUTC). # # - rp: The RavenPack table. Each line is an \"event\" (see the RP manuals for how RP defines an # entity and an event) # # Example of ciq: # gvkey rp_entity_id rdq ciq_ann_dttm_utc # 1: 014535 BDE867 2021-05-07 2021-05-07 11:30:00 # # Example of rp: # rp_entity_id rp_story_id rp_dttm_utc source_name # 1: D502AB E75346353C34D9DA358AB6F344108EF9 2007-12-21 13:07:59 Canadian News Wire # ---- Step 1: split ciq into two tables ---- # ciqd: only has date information (74%) # ciqdt: has both date and time information (26%) ciqd = ciq[as_hms(ciq_ann_dttm_utc)==0] ciqdt = ciq[as_hms(ciq_ann_dttm_utc)!=0] dobs(ciqdt, ciqd) # ---- Step 2: Process ciqd ---- # assign default timestamp: 23:59:59 on RDQ day ciqd[, ':='(joindttm=as_datetime(rdq, 'America/New_York')+as.numeric(as_hms('23:59:59')))] ciqd_rp_1 = rp[, ':='(joindttm=rp_dttm_utc) # query previous 2 days ][ciqd, on=.(rp_entity_id, joindttm), roll=2*24*3600, nomatch=NULL # headlines should not include \"to report\" -- these news are # not the actually earnings results, but just a notification that # the company will release earnings soon. ][!str_detect(headline, regex('to report', ignore_case=T)) # keep the earliest RP timestamp ][order(gvkey, rdq, rp_dttm_utc) ][, .SD[1], keyby=.(gvkey, rdq) ][, .(gvkey, rdq, rp_dttm_utc, rp_story_id, headline)] # merge ciqd_rp back to ciq ciqd_rp_2 = ciqd_rp_1[ciqd, on=.(gvkey, rdq), nomatch=NA ][, ':='(joindttm=NULL)] # ---- step 3: Match ciqdt ---- # query previous 3 hours ciqdt_rp_locf = rp[, ':='(joindttm=rp_dttm_utc) # search for the previous 3 hours in RP ][ciqdt[, ':='(joindttm=ciq_ann_dttm_utc)], on=.(rp_entity_id, joindttm), roll=3*3600, nomatch=NULL # keep the earliest RP timestamp ][order(gvkey, rdq, rp_dttm_utc) ][, .SD[1], keyby=.(gvkey, rdq) ][, .(gvkey, rdq, rp_dttm_utc, rp_story_id, headline, jointype='match_prev')] # query next 60 seconds ciqdt_rp_nocb = rp[, ':='(joindttm=rp_dttm_utc) # search for the next 60 seconds (to deal with the flooring issue) ][ciqdt[, ':='(joindttm=ciq_ann_dttm_utc)], on=.(rp_entity_id, joindttm), roll=-60, nomatch=NULL # keep the earliest RP timestamp ][order(gvkey, rdq, rp_dttm_utc) ][, .SD[1], keyby=.(gvkey, rdq) ][, .(gvkey, rdq, rp_dttm_utc, rp_story_id, headline, jointype='match_next')] # combine results ciqdt_rp_1 = rbindlist(list(ciqdt, ciqdt_rp_locf, ciq_rpdt_nocb), use.names=T, fill=T) ciqdt_rp_1 = ciqdt_rp_1[!str_detect(headline, regex('to report', ignore_case=T)), .(gvkey, rdq, rp_dttm_utc, rp_story_id, headline, jointype)] # determine which RP timestamp to use ciqdt_rp_2 = ciqdt_rp_1[order(gvkey, rdq, jointype) ][, { # if no match, keep original CIQ timestamp if (all(is.na(jointype))) { out = .SD[1] # if at least matches to one earlier news article in RP, # use the RP timestamp } else if ('match_prev' %in% jointype) { out = .SD[jointype=='match_prev'][1] # if only matches to a RP timestamp no later than 60s, # use the RP timestamp } else if ('match_next' %in% jointype) { out = .SD[jointype=='match_next'][1] } out }, keyby=.(gvkey, rdq) ][, .(gvkey, rdq, rp_dttm_utc, rp_story_id, headline)] # merge ciqdt_rp back to ciq ciqdt_rp_3 = ciqdt_rp_2[ciqdt, on=.(gvkey, rdq), nomatch=NA ][, ':='(joindttm=NULL)] # if rp_dttm_utc and ciq_ann_dttm_utc are the same, then # assign rp_dttm_utc a NA # # in the final","date":"2023-01-25","objectID":"/nail-down-earnings-time/:6:2","series":["WRDS Tips"],"tags":["wrds","ravenpack"],"title":"Nailing down the earnings event time to the millisecond\n","uri":"/nail-down-earnings-time/#r-code"},{"categories":null,"content":" The issueI found the filing and accepted date in WRDS SEC Analytics doesn’t agree with EDGAR. For example, the following 8-K event (cik=“0000003116”) has an accepted date of 2003-7-7 (EDGAR link): But in WRDS SEC Analytics Suite, the accepted date (secadate) is 2003-7-4 while the accepted time (secatime) and report time (rdate) are correctly parsed: ","date":"2023-01-25","objectID":"/accepted-date-not-match-edgar/:0:0","series":["WRDS Tips"],"tags":["wrds","sec"],"title":"The \"Accepted Date\" from WRDS SEC suite doesn't match EDGAR\n","uri":"/accepted-date-not-match-edgar/#the-issue"},{"categories":null,"content":" WRDS’s responseWRDS confirms my findings: Quote You are correct in your findings. For older filings, the tag \u003cACCEPTANCE-DATETIME\u003e doesn’t always exist in the raw text. Therefore, we use the date on the \u003cSEC-HEADER\u003e row for older filings. When I asked “how old is old,” WRDS further adds: Quote EDGAR and the raw filing are one in the same. Thus, it’s not an issue of whether they match or not. Rather, accepted datetime is not a reliable field for old filings. I refer to old filings as 2003 or earlier. Thus, you’ll start to see some filings with accepted datetime populated in 2003 as the SEC started to move to an electronic system. However, most filings in 2003 and earlier will not have this information, and even those that do may not be reliable. A more accurate value for the accepted time comes from the \u003cSEC-HEADER\u003e row for these filings. My understanding is: We should not trust “accepted date/time” in and before 2003. When we see a discrepency between WRDS and EDGAR for these old filings, WRDS might be more trustable. ","date":"2023-01-25","objectID":"/accepted-date-not-match-edgar/:0:0","series":["WRDS Tips"],"tags":["wrds","sec"],"title":"The \"Accepted Date\" from WRDS SEC suite doesn't match EDGAR\n","uri":"/accepted-date-not-match-edgar/#wrdss-response"},{"categories":null,"content":"CCM is a product provided by CRSP to link the Compustat database (gvkey) to CRSP (permno). In CCM, there’re three tables you can use to make the link: ccmxpf_linktable ccmxpf_lnkhist ccmxpf_lnkused Which one should you use? ","date":"2023-01-24","objectID":"/how-to-use-ccm/:0:0","series":["WRDS Tips"],"tags":["wrds","crsp"],"title":"The correct way to use CCM","uri":"/how-to-use-ccm/#"},{"categories":null,"content":" TL;DR WRDS recommend using ccmxpf_lnkhist: Quote “The link history table (CCMXPF_LNKHIST) is the main table used for WRDS CCM web queries. It provides access to all Compustat records, regardless of whether or not the securities are in the CRSP universe.” ccmxpf_lnktable is deprecated and should not be used. Before Feb 2014, ccmxpf_lnktable is used to add usedflag to ccmxpf_lnkused. But after Feb 2014 usedflag is no longer needed, hence ccmxpf_lnktable become meaningless: Quote “SAS programmers will also notice a change. Since Used Flag is no longer needed the dataset CCMXPF_LINKTABLE is also not needed. This table added Used Flag from CCMXPF_LNKUSED to the data in CCMXPF_LNKHIST. The Link History table is “Compustat-centric” - it has all companies, whether or not a link exists to a security in CRSP. It has all the companies in the Link Used table and more. We recommend using the Link History dataset directly.” ccmxpf_lnkused is safe to use, but it’s CRSP centric, meaning it has a smaller stock universe than ccmxpf_lnkhist. For most of the time, you should use ccmxpf_lnkhist instead of ccmxpf_lnkused. Quote “The link used table (CCMXPF_LNKUSED) has all records of GVKEYs with a match to a PERMNO identifier, even those not used. This table is CRSP-centric (i.e. starting with PERMNO), compared to the link history table which is Compustat-centric (i.e. starting with GVKEY). This table allows users searching by PERMNO to find all possible GVKEYs that have ever been linked, including those not used during a certain period.” ","date":"2023-01-24","objectID":"/how-to-use-ccm/:0:0","series":["WRDS Tips"],"tags":["wrds","crsp"],"title":"The correct way to use CCM","uri":"/how-to-use-ccm/#tldr"},{"categories":null,"content":" Clean ccmxpf_lnkhistWhile lnkhist is the go-to table, it has an issue of “consecutive observations.” As an illustration, see the following example: gvkey lpermno lpermco linkdt linkenddt 1 001010 10006 22156 1950-05-01 1962-01-30 2 001010 10006 22156 1962-01-31 1984-06-28 As you can see, line 1 and 2 should be merged into a single line: gvkey lpermno lpermco linkdt linkenddt 1 001010 10006 22156 1950-05-01 1981-06-28 WRDS provides an SAS example to “collapse” the consecutive observations (yeah I know the question “who’s still using SAS” is like “who’s still buying GTA5”). I translate the SAS code to R library(data.table) # get the \"ccmxpf_lnkhist\" table # make sure ccm is a data.table ccm = get_ccmxpf_lnkhist() # create previous linkenddt and linkdt ccm = ccm[, ':='(last_linkdt=shift(linkdt), last_linkenddt=shift(linkenddt)), keyby=.(gvkey, lpermno)] # merge consecutive observations ccm = ccm[ # if linkdt is on the same day or 1 day after last_linkenddt, # rewrite linkdt to last_linkdt linkdt==last_linkenddt+1 | linkdt==last_linkenddt, ':='(linkdt=last_linkdt) # only keep the last observation of (gvkey, lpermno, linkdt) pair ][order(gvkey, lpermno, linkdt, linkenddt) ][, .SD[.N], keyby=.(gvkey, lpermno, linkdt) ][, c('last_linkdt', 'last_linkenddt') := NULL] ","date":"2023-01-24","objectID":"/how-to-use-ccm/:0:0","series":["WRDS Tips"],"tags":["wrds","crsp"],"title":"The correct way to use CCM","uri":"/how-to-use-ccm/#clean-ccmxpf_lnkhist"},{"categories":null,"content":" Query the interval: foverlapWe often need to query overlaps. For example, given a table of daily trading volume, we want to find for each day t, the trading volume of its surrounding days, i.e., [t-N,t+N]. Example: Suppose we have an event table. The event table contains the event date. Events are infrequent. In our example, events only take place on day 1, 3, 5: # event: infrequent events of interests event = data.table(date=c(1,3,5)) event #\u003e date #\u003e 1: 1 #\u003e 2: 3 #\u003e 3: 5 There’s another table, called history. The history table records values of interest on each day. In the example, the history table has a value for each day from 1 to 5: # history: chronical records *for each date*. history = data.table(date=1:5, value=sample(letters, 5, replace=TRUE)) history #\u003e date value #\u003e 1: 1 v #\u003e 2: 2 s #\u003e 3: 3 j #\u003e 4: 4 p #\u003e 5: 5 p Our goal: For each date t, find the value in the range of [t-1,t+1]: # our goal: #\u003e date value #\u003e 1: 1 v,s #\u003e 2: 3 s,j,p #\u003e 3: 5 p,p To achieve our goal, follow these steps: # Step 1: add start/end to event event[, ':='(start=date-1, end=date+1)] setkey(event, start, end) # event *must* be keyed! event #\u003e date start end #\u003e 1: 1 0 2 #\u003e 2: 3 2 4 #\u003e 3: 5 4 6 # Step 2: add start/end to history history[, ':='(start=date, end=date)] setkey(history, start, end) # history is *recommended* to be keyed history #\u003e date value start end #\u003e 1: 1 v 1 1 #\u003e 2: 2 s 2 2 #\u003e 3: 3 j 3 3 #\u003e 4: 4 p 4 4 #\u003e 5: 5 p 5 5 # Step 3: run foverlaps! out = foverlaps(history, event, nomatch=NULL) out[, .(value=list(value)), keyby=.(date)] #\u003e date value #\u003e 1: 1 v,s #\u003e 2: 3 s,j,p #\u003e 3: 5 p,p ","date":"2023-01-23","objectID":"/foverlap-and-roll/:0:0","series":["R and datatable"],"tags":["r","data.table"],"title":"How to query an interval? Understand `foverlap` and `roll` in data.table","uri":"/foverlap-and-roll/#query-the-interval-foverlap"},{"categories":null,"content":" Query the fuzzy point: rollSometimes we don’t need to find all surrounding days for the event date. Instead, we only need to find the “nearest” day for the event date. This is when roll comes in. Example: Let’s say this time we only have one event at day 2: event = data.table(date=c(2)) #\u003e date #\u003e 1: 2 The history table, contains 5 obs: history = data.table(date=c(1, 3, 5, 8, 9), value=sample(letters, 5, replace=TRUE)) #\u003e date value #\u003e 1: 1 r #\u003e 2: 3 b #\u003e 3: 5 g #\u003e 4: 8 e #\u003e 5: 9 g Our goal: find the matched obs 4 days after the event (i.e., day 6) Step 1: Add the join_date to event and history # Add join_date to the event table event[, ':='(join_date=date+4)] #\u003e date join_date #\u003e 1: 2 6 # Add join_date to the history table history[, ':='(join_date=date)] # the join_date is the same as date! #\u003e date value join_date #\u003e 1: 1 r 1 #\u003e 2: 3 b 3 #\u003e 3: 5 g 5 #\u003e 4: 8 e 8 #\u003e 5: 9 g 9 Step 2: Use event to “query” history. The results vary depending on the roll options # Case 1: closest: the closest to \"6\" is \"5\" history[event, on=.(join_date), roll='nearest'][, .(date, value)] #\u003e date value #\u003e 1: 5 g # Case 2: Inf: match the *previling* value (LOCF). The previling value of \"6\" is \"5\" history[event, on=.(join_date), roll=Inf][, .(date, value)] #\u003e date value #\u003e 1: 5 g # Case 3: -Inf: match the *next* value (NOCB). The next value of \"6\" is \"8\" history[event, on=.(join_date), roll=-Inf][, .(date, value)] #\u003e date value #\u003e 1: 8 e # Case 4: Positive: match the previling at no more than \"roll\" days # The previling value of \"6\" is \"5\", which is within 1 day history[event, on=.(join_date), roll=1][, .(date, value)] #\u003e date value #\u003e 1: 5 g # Case 5: Negative: match the next at no more than \"roll\" days # The next no-more-than-one value of \"6\" is \"7\", match *failed* history[event, on=.(join_date), roll=-1][, .(date, value)] #\u003e date value #\u003e 1: NA \u003cNA\u003e The unit of `roll` The unit of roll (when specified as a number) depends on the unit of join_date. If join_date is Date, the unit is day, i.e., roll=5 means LOCF no more than 5 days. If join_date is POSIXct, the unit is second, i.e., roll=5 means LOCF no more than 5 seconds. `foverlap` vs. `roll`: When to use which? Both methods are fast! foverlaps Pros: allows for precise matching could match to multiple values Cons: can’t do fuzzy matching more complicated roll Pros: could do fuzzy matching less code Cons: couldn’t do precise matching (can only specify a range) only matches to one value ","date":"2023-01-23","objectID":"/foverlap-and-roll/:0:0","series":["R and datatable"],"tags":["r","data.table"],"title":"How to query an interval? Understand `foverlap` and `roll` in data.table","uri":"/foverlap-and-roll/#query-the-fuzzy-point-roll"},{"categories":null,"content":" 1 Parallelization in RTL;DR fun \u003c- function(...) { # define your function } # init library(future.apply) plan(multicore, 32) # in Mac, choose multicore # run! future_lapply(iterable, fun) future needs a little bit time to pick. I’ve seen cases where processing 1000 items used 20 s, and processing 5000 still uses 20 s! ","date":"2023-01-23","objectID":"/parallel-in-r/:0:0","series":["R and datatable"],"tags":["r","data.table"],"title":"Parallelization in R","uri":"/parallel-in-r/#1-parallelization-in-r"}]